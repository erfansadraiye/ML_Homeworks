{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IDMsUWXxWAY0"
      },
      "source": [
        "<div>\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/6/64/Sharif_University_Logo.jpg' alt=\"SUT logo\" width=220 height=220 align=left class=\"saturate\">\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center>\n",
        "<!-- <font color=0F5298 size=7> -->\n",
        "<font color=0F5298 size=6>\n",
        "    Introduction to Machine Learning <br> <br>\n",
        "<!-- <font color=2565AE size=5> -->\n",
        "<font size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023 <br> <br>\n",
        "<font color=606060 size=5>\n",
        "    Project\n",
        "\n",
        "____"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LFnV5KF1WEdQ"
      },
      "source": [
        "### Full Name : Erfan Sadraiye\n",
        "### Student Number : 99101835\n",
        "### Colab Link:\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENNanWBgWBP1",
        "outputId": "f712f813-95b0-45d2-96f9-342462148981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.10.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install fasttext"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHgkAq1qZU3"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLdIkzthVo6M"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import fasttext\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from time import time\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import  classification_report\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ghr1f7Omqaqz"
      },
      "source": [
        "Download nltk files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE9fsQIzXYB0",
        "outputId": "89fa274b-9df1-4a58-d572-948f026dcfa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yPHs7sRIsM"
      },
      "source": [
        "Reading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsUSNws6W-Th"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/data-train.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hA-auKemRNeb"
      },
      "source": [
        "Preprocess data\n",
        "- tokenize\n",
        "- lower case text\n",
        "- remove punctuation\n",
        "- stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGpCbOpxXFKH"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, stopword_removal=True, lower_case=True,\n",
        "                       punctuation_removal=True , porter_stemmer=ps):\n",
        "    normalized_tokens = word_tokenize(text)\n",
        "    if lower_case:\n",
        "        normalized_tokens = [word.lower() for word in normalized_tokens]\n",
        "    if punctuation_removal:\n",
        "        normalized_tokens = [word for word in normalized_tokens if word not in string.punctuation]\n",
        "    if stopword_removal:\n",
        "        stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
        "        normalized_tokens = [word for word in normalized_tokens if word.lower() not in stopwords]\n",
        "    normalized_tokens = [ps.stem(word) for word in normalized_tokens]\n",
        "    return normalized_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSYzJPgaXV-O"
      },
      "outputs": [],
      "source": [
        "df['preprocessed_phrases'] = df['Phrase'].apply(lambda x: preprocess_text(x))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeNkClweqqKq"
      },
      "source": [
        "split data to 3 data set\n",
        "- Training 80%\n",
        "- Validation 10%\n",
        "- Test 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQFoQsYVYI9_"
      },
      "outputs": [],
      "source": [
        "train_df, remaining_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(remaining_df, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iK82EAOpONl"
      },
      "outputs": [],
      "source": [
        "train_phrases = train_df['preprocessed_phrases'].apply(lambda x: ' '.join(x))\n",
        "val_phrases = val_df['preprocessed_phrases'].apply(lambda x: ' '.join(x))\n",
        "test_phrases = test_df['preprocessed_phrases'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "y_train = train_df['Sentiment'].values\n",
        "y_val = val_df['Sentiment'].values\n",
        "y_test = test_df['Sentiment'].values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "auOuFZwXqw0a"
      },
      "source": [
        "Fit TF-ITF on trainig phrases to convert each phrase to a vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "jfw28jvKYC8d",
        "outputId": "553f9917-496e-43d9-dfea-3175afff597b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "TfidfVectorizer()"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_idf = TfidfVectorizer()\n",
        "tf_idf.fit(train_phrases)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHdu8-ozYrej"
      },
      "outputs": [],
      "source": [
        "vectorized_train = tf_idf.transform(train_phrases).toarray()\n",
        "vectorized_val = tf_idf.transform(val_phrases).toarray()\n",
        "vectorized_test = tf_idf.transform(test_phrases).toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1LvH1MBzsIt",
        "outputId": "f7f02f0d-e203-4871-a55a-54a521892442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cSqG2tM2q4U-"
      },
      "source": [
        "Fit a Neural Network on our vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-4J7L16vcno"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x: list, y: list):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        for size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, size))\n",
        "            layers.append(nn.BatchNorm1d(size))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_size = size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "        layers.append(nn.Softmax())\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(torch.float32)\n",
        "        out = self.model(x)\n",
        "        return out\n",
        "\n",
        "class NeuralNetworkTrainer:\n",
        "     def __init__(self, input_size, hidden_sizes, output_size, learning_rate=0.001, num_epochs=50):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = NeuralNetwork(input_size, hidden_sizes, output_size).to(self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        self.best_model = None\n",
        "        self.best_accuracy = 0.0\n",
        "\n",
        "\n",
        "     def train_nn(self, x_train, y_train, x_val=None, y_val=None, batch_size=64):\n",
        "        dataset = CustomDataset(x_train, y_train)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=1)\n",
        "\n",
        "        if x_val is not None and y_val is not None:\n",
        "            val_dataset = CustomDataset(x_val, y_val)\n",
        "            val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=1)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            # Training\n",
        "            train_loss, train_accuracy = self.train_epoch(dataloader)\n",
        "\n",
        "            # Validation\n",
        "            if x_val is not None and y_val is not None:\n",
        "                val_loss, val_accuracy = self.validate(val_dataloader)\n",
        "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "                      f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "                if val_accuracy > self.best_accuracy:\n",
        "                    self.best_accuracy = val_accuracy\n",
        "                    self.best_model = self.model.state_dict().copy()\n",
        "            else:\n",
        "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "        if self.best_model is not None:\n",
        "            self.model.load_state_dict(self.best_model)\n",
        "\n",
        "\n",
        "     def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with tqdm(total=len(dataloader), ncols=80) as pbar:\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.update(1)\n",
        "                pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_accuracy = correct / total\n",
        "\n",
        "        return epoch_loss, epoch_accuracy\n",
        "\n",
        "     def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_accuracy = correct / total\n",
        "\n",
        "        return epoch_loss, epoch_accuracy\n",
        "\n",
        "     def predict(self, x_test, batch_size=64):\n",
        "        dataset = CustomDataset(x_test, [0] * len(x_test))\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=1)\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, _ in dataloader:\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "                predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cC6vl0uzKAm",
        "outputId": "de7b5bfa-27c1-4d7d-83f3-0b302f549857"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                  | 0/2423 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n",
            "Loss: 1.2479: 100%|█████████████████████████| 2423/2423 [01:22<00:00, 29.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Train Loss: 1.3585, Train Accuracy: 0.5417, Validation Loss: 1.3038, Validation Accuracy: 0.5974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3432: 100%|█████████████████████████| 2423/2423 [01:13<00:00, 33.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20, Train Loss: 1.3013, Train Accuracy: 0.6001, Validation Loss: 1.2740, Validation Accuracy: 0.6287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3568: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20, Train Loss: 1.2772, Train Accuracy: 0.6249, Validation Loss: 1.2546, Validation Accuracy: 0.6482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2590: 100%|█████████████████████████| 2423/2423 [01:15<00:00, 31.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20, Train Loss: 1.2626, Train Accuracy: 0.6399, Validation Loss: 1.2454, Validation Accuracy: 0.6576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2121: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20, Train Loss: 1.2528, Train Accuracy: 0.6501, Validation Loss: 1.2362, Validation Accuracy: 0.6672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3161: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20, Train Loss: 1.2442, Train Accuracy: 0.6588, Validation Loss: 1.2282, Validation Accuracy: 0.6756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2358: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/20, Train Loss: 1.2371, Train Accuracy: 0.6663, Validation Loss: 1.2241, Validation Accuracy: 0.6798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.0763: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20, Train Loss: 1.2337, Train Accuracy: 0.6701, Validation Loss: 1.2227, Validation Accuracy: 0.6810\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2802: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20, Train Loss: 1.2298, Train Accuracy: 0.6739, Validation Loss: 1.2179, Validation Accuracy: 0.6862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.0823: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20, Train Loss: 1.2265, Train Accuracy: 0.6773, Validation Loss: 1.2151, Validation Accuracy: 0.6889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2551: 100%|█████████████████████████| 2423/2423 [01:12<00:00, 33.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20, Train Loss: 1.2246, Train Accuracy: 0.6792, Validation Loss: 1.2151, Validation Accuracy: 0.6890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3474: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20, Train Loss: 1.2233, Train Accuracy: 0.6806, Validation Loss: 1.2117, Validation Accuracy: 0.6925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2812: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20, Train Loss: 1.2208, Train Accuracy: 0.6832, Validation Loss: 1.2105, Validation Accuracy: 0.6937\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3210: 100%|█████████████████████████| 2423/2423 [01:12<00:00, 33.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/20, Train Loss: 1.2179, Train Accuracy: 0.6861, Validation Loss: 1.2096, Validation Accuracy: 0.6947\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.1545: 100%|█████████████████████████| 2423/2423 [01:12<00:00, 33.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/20, Train Loss: 1.2169, Train Accuracy: 0.6872, Validation Loss: 1.2081, Validation Accuracy: 0.6959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2167: 100%|█████████████████████████| 2423/2423 [01:12<00:00, 33.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/20, Train Loss: 1.2149, Train Accuracy: 0.6894, Validation Loss: 1.2062, Validation Accuracy: 0.6981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2044: 100%|█████████████████████████| 2423/2423 [01:12<00:00, 33.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/20, Train Loss: 1.2142, Train Accuracy: 0.6898, Validation Loss: 1.2051, Validation Accuracy: 0.6991\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.1795: 100%|█████████████████████████| 2423/2423 [01:12<00:00, 33.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/20, Train Loss: 1.2122, Train Accuracy: 0.6917, Validation Loss: 1.2029, Validation Accuracy: 0.7013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3241: 100%|█████████████████████████| 2423/2423 [01:12<00:00, 33.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/20, Train Loss: 1.2102, Train Accuracy: 0.6940, Validation Loss: 1.2016, Validation Accuracy: 0.7028\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.1554: 100%|█████████████████████████| 2423/2423 [01:11<00:00, 33.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/20, Train Loss: 1.2089, Train Accuracy: 0.6951, Validation Loss: 1.2004, Validation Accuracy: 0.7040\n"
          ]
        }
      ],
      "source": [
        "input_size, hidden_sizes, output_size = len(vectorized_train[0]), [4096, 1024, 128], 5\n",
        "\n",
        "nn_trainer_cv = NeuralNetworkTrainer(input_size, hidden_sizes, output_size,learning_rate=0.01,num_epochs=20)\n",
        "nn_trainer_cv.train_nn(vectorized_train, y_train, vectorized_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNvO6Ip6u5iw",
        "outputId": "922ae17c-6e07-41e4-fccb-e33c2c6c0b00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       705\n",
            "           1       0.64      0.59      0.62      2782\n",
            "           2       0.75      0.91      0.82      7817\n",
            "           3       0.63      0.67      0.65      3262\n",
            "           4       0.00      0.00      0.00       939\n",
            "\n",
            "    accuracy                           0.70     15505\n",
            "   macro avg       0.40      0.43      0.42     15505\n",
            "weighted avg       0.62      0.70      0.66     15505\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "preds = nn_trainer_cv.predict(vectorized_test)\n",
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, preds))\n",
        "print(\"Recall Macro: \", recall_score(y_test, preds, average='macro'))\n",
        "print(\"Recall Micro: \", recall_score(y_test, preds, average='micro'))\n",
        "print(\"Precision Macro: \", precision_score(y_test, preds, average='macro'))\n",
        "print(\"Precision Micro: \", precision_score(y_test, preds, average='micro'))\n",
        "print(\"F1 Macro: \", f1_score(y_test, preds, average='macro'))\n",
        "print(\"F1 Micro: \", f1_score(y_test, preds, average='micro'))\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "osdf3iFDvPGB"
      },
      "source": [
        "# Failed Approachs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ArIbzEf-Y5T3",
        "outputId": "cd3125ac-5530-4ae7-ab37-2a5c67f64d26"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(vectorized_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nUQUng_FVI0R"
      },
      "outputs": [],
      "source": [
        "preds = clf.predict(vectorized_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MoJKWhgY0E1",
        "outputId": "3f2830e4-a057-48a2-c170-0c69ece73d11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6259916156078684\n",
            "Recall Macro:  0.42830763995924076\n",
            "Recall Micro:  0.6259916156078684\n",
            "Precision Macro:  0.5802413761272678\n",
            "Precision Micro:  0.6259916156078684\n",
            "F1 Macro:  0.4655752216953564\n",
            "F1 Micro:  0.6259916156078684\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.19      0.29       705\n",
            "           1       0.54      0.36      0.43      2782\n",
            "           2       0.67      0.87      0.75      7817\n",
            "           3       0.55      0.47      0.51      3262\n",
            "           4       0.57      0.25      0.34       939\n",
            "\n",
            "    accuracy                           0.63     15505\n",
            "   macro avg       0.58      0.43      0.47     15505\n",
            "weighted avg       0.61      0.63      0.60     15505\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Accuracy: \", accuracy_score(y_test, preds))\n",
        "print(\"Recall Macro: \", recall_score(y_test, preds, average='macro'))\n",
        "print(\"Recall Micro: \", recall_score(y_test, preds, average='micro'))\n",
        "print(\"Precision Macro: \", precision_score(y_test, preds, average='macro'))\n",
        "print(\"Precision Micro: \", precision_score(y_test, preds, average='micro'))\n",
        "print(\"F1 Macro: \", f1_score(y_test, preds, average='macro'))\n",
        "print(\"F1 Micro: \", f1_score(y_test, preds, average='micro'))\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USKUsN616qpe",
        "outputId": "4e36be26-f70f-4a07-cf8b-6aff8071fa38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0).fit(vectorized_train, y_train)\n",
        "y_pred_log = clf.predict(vectorized_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xL57cB_aLby",
        "outputId": "a6deb24f-231e-4c91-e34c-c466e9d226b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6259916156078684\n",
            "Recall Macro:  0.42830763995924076\n",
            "Recall Micro:  0.6259916156078684\n",
            "Precision Macro:  0.5802413761272678\n",
            "Precision Micro:  0.6259916156078684\n",
            "F1 Macro:  0.4655752216953564\n",
            "F1 Micro:  0.6259916156078684\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.19      0.29       705\n",
            "           1       0.54      0.36      0.43      2782\n",
            "           2       0.67      0.87      0.75      7817\n",
            "           3       0.55      0.47      0.51      3262\n",
            "           4       0.57      0.25      0.34       939\n",
            "\n",
            "    accuracy                           0.63     15505\n",
            "   macro avg       0.58      0.43      0.47     15505\n",
            "weighted avg       0.61      0.63      0.60     15505\n",
            "\n",
            "Accuracy:  0.6259916156078684\n",
            "Recall Macro:  0.42830763995924076\n",
            "Recall Micro:  0.6259916156078684\n",
            "Precision Macro:  0.5802413761272678\n",
            "Precision Micro:  0.6259916156078684\n",
            "F1 Macro:  0.4655752216953564\n",
            "F1 Micro:  0.6259916156078684\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.19      0.29       705\n",
            "           1       0.54      0.36      0.43      2782\n",
            "           2       0.67      0.87      0.75      7817\n",
            "           3       0.55      0.47      0.51      3262\n",
            "           4       0.57      0.25      0.34       939\n",
            "\n",
            "    accuracy                           0.63     15505\n",
            "   macro avg       0.58      0.43      0.47     15505\n",
            "weighted avg       0.61      0.63      0.60     15505\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred_log))\n",
        "print(\"Recall Macro: \", recall_score(y_test, y_pred_log, average='macro'))\n",
        "print(\"Recall Micro: \", recall_score(y_test, y_pred_log, average='micro'))\n",
        "print(\"Precision Macro: \", precision_score(y_test, y_pred_log, average='macro'))\n",
        "print(\"Precision Micro: \", precision_score(y_test, y_pred_log, average='micro'))\n",
        "print(\"F1 Macro: \", f1_score(y_test, y_pred_log, average='macro'))\n",
        "print(\"F1 Micro: \", f1_score(y_test, y_pred_log, average='micro'))\n",
        "print(classification_report(y_test, y_pred_log))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CHiUAXIMUfMf"
      },
      "source": [
        "FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "126Wy-8HUg5l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import fasttext\n",
        "\n",
        "class FastTextSentenceVectorizer:\n",
        "\n",
        "    def __init__(self, method='skipgram', model_path=None):\n",
        "        self.method = method\n",
        "        self.model = None\n",
        "        self.model_path = model_path\n",
        "\n",
        "    def train(self, texts):\n",
        "        \"\"\"\n",
        "        train the fasttext model and save it into self.model\n",
        "        Parameters\n",
        "        ----------\n",
        "        texts: list of list of str\n",
        "        \"\"\"\n",
        "        file_name = 'data.txt'\n",
        "        with open(file_name, 'w') as f:\n",
        "            for item in texts:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "            f.close()\n",
        "        model = fasttext.train_unsupervised('data.txt', model=self.method)\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def load_model(self):\n",
        "        self.model = fasttext.load_model(self.model_path)\n",
        "\n",
        "    def vectorize(self, sentences):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"FastText model is not trained or loaded. Please call 'train()' or 'load_model()' first.\")\n",
        "\n",
        "        vectors = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            vector = self.model.get_sentence_vector(sentence)\n",
        "            vectors.append(vector)\n",
        "\n",
        "        return np.vstack(vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdjzi7zzbfbr"
      },
      "outputs": [],
      "source": [
        "ftv = FastTextSentenceVectorizer()\n",
        "ftv.train(train_phrases.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tGjptpLfam2",
        "outputId": "76142542-b479-47bc-95bd-5fc63208bb94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0.9083881974220276, 'fantasti'), (0.9080108404159546, 'fantas'), (0.904706597328186, 'fantasma'), (0.897767961025238, 'fantasia'), (0.8972718119621277, 'fantasi'), (0.8403797149658203, 'fantast'), (0.83005690574646, 'phantasm'), (0.8067594170570374, 'farcic'), (0.7783586978912354, 'ecstasi'), (0.7619239091873169, 'fanboy')]\n",
            "[(0.8908090591430664, 'rueful'), (0.8890059590339661, 'cumul'), (0.8799886107444763, 'awe'), (0.8761646747589111, 'symbiot'), (0.874527633190155, 'true-to-lif'), (0.8683801889419556, 'pelosi'), (0.8664852380752563, 'larger-than-lif'), (0.8641816973686218, 'near-hypnot'), (0.8630688190460205, 'full-fledg'), (0.8617079854011536, 'abysm')]\n"
          ]
        }
      ],
      "source": [
        "print(ftv.model.get_nearest_neighbors('fantastic'))\n",
        "print(ftv.model.get_nearest_neighbors('awful'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmNvexmpbqUC"
      },
      "outputs": [],
      "source": [
        "train_vectors = ftv.vectorize(train_phrases.values)\n",
        "val_vectors = ftv.vectorize(val_phrases.values)\n",
        "test_vectors = ftv.vectorize(test_phrases.values)\n",
        "xxxx = ftv.vectorize(df['Phrase'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjjcIkuHc62r",
        "outputId": "2df9259c-609f-44e5-f164-6cb411d45c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.508287649145437\n",
            "Recall Macro:  0.22753468942302804\n",
            "Recall Micro:  0.508287649145437\n",
            "Precision Macro:  0.406227455312127\n",
            "Precision Micro:  0.508287649145437\n",
            "F1 Macro:  0.2037122577775739\n",
            "F1 Micro:  0.508287649145437\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.01      0.01       680\n",
            "           1       0.35      0.06      0.11      2680\n",
            "           2       0.53      0.91      0.67      7917\n",
            "           3       0.38      0.15      0.21      3295\n",
            "           4       0.20      0.01      0.02       933\n",
            "\n",
            "    accuracy                           0.51     15505\n",
            "   macro avg       0.41      0.23      0.20     15505\n",
            "weighted avg       0.45      0.51      0.41     15505\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import  classification_report\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(train_vectors, y_train)\n",
        "preds = clf.predict(val_vectors)\n",
        "print(\"Accuracy: \", accuracy_score(y_val, preds))\n",
        "print(\"Recall Macro: \", recall_score(y_val, preds, average='macro'))\n",
        "print(\"Recall Micro: \", recall_score(y_val, preds, average='micro'))\n",
        "print(\"Precision Macro: \", precision_score(y_val, preds, average='macro'))\n",
        "print(\"Precision Micro: \", precision_score(y_val, preds, average='micro'))\n",
        "print(\"F1 Macro: \", f1_score(y_val, preds, average='macro'))\n",
        "print(\"F1 Micro: \", f1_score(y_val, preds, average='micro'))\n",
        "print(classification_report(y_val, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_4WKQ0mec7M",
        "outputId": "c9685987-70c0-4976-f1b8-f21597926917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.19      0.28       705\n",
            "           1       0.49      0.28      0.36      2782\n",
            "           2       0.62      0.87      0.72      7817\n",
            "           3       0.49      0.35      0.41      3262\n",
            "           4       0.48      0.16      0.24       939\n",
            "\n",
            "    accuracy                           0.58     15505\n",
            "   macro avg       0.52      0.37      0.40     15505\n",
            "weighted avg       0.55      0.58      0.54     15505\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "n_samples = 50000\n",
        "rf_classifier.fit(train_vectors[:n_samples], y_train[:n_samples])\n",
        "\n",
        "y_pred = rf_classifier.predict(test_vectors)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1Usq4xJc-J1",
        "outputId": "53070f45-af64-48ff-cfde-5a823ed2bc11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2936: 100%|████████████████████████| 1939/1939 [00:14<00:00, 131.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30, Train Loss: 1.3497, Train Accuracy: 0.5485, Validation Loss: 1.3339, Validation Accuracy: 0.5606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2936: 100%|████████████████████████| 1939/1939 [00:18<00:00, 105.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/30, Train Loss: 1.3285, Train Accuracy: 0.5674, Validation Loss: 1.3310, Validation Accuracy: 0.5636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2284: 100%|████████████████████████| 1939/1939 [00:14<00:00, 131.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/30, Train Loss: 1.3200, Train Accuracy: 0.5768, Validation Loss: 1.3288, Validation Accuracy: 0.5668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4929: 100%|████████████████████████| 1939/1939 [00:13<00:00, 145.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/30, Train Loss: 1.3148, Train Accuracy: 0.5830, Validation Loss: 1.3268, Validation Accuracy: 0.5688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4082: 100%|████████████████████████| 1939/1939 [00:13<00:00, 146.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/30, Train Loss: 1.3106, Train Accuracy: 0.5873, Validation Loss: 1.3238, Validation Accuracy: 0.5716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3734: 100%|████████████████████████| 1939/1939 [00:13<00:00, 140.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/30, Train Loss: 1.3068, Train Accuracy: 0.5921, Validation Loss: 1.3239, Validation Accuracy: 0.5723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3102: 100%|████████████████████████| 1939/1939 [00:13<00:00, 141.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/30, Train Loss: 1.3030, Train Accuracy: 0.5961, Validation Loss: 1.3207, Validation Accuracy: 0.5781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.3105: 100%|████████████████████████| 1939/1939 [00:13<00:00, 143.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/30, Train Loss: 1.2998, Train Accuracy: 0.5994, Validation Loss: 1.3208, Validation Accuracy: 0.5758\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.0897: 100%|████████████████████████| 1939/1939 [00:13<00:00, 142.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/30, Train Loss: 1.2968, Train Accuracy: 0.6028, Validation Loss: 1.3190, Validation Accuracy: 0.5770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4099: 100%|████████████████████████| 1939/1939 [00:13<00:00, 144.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/30, Train Loss: 1.2942, Train Accuracy: 0.6060, Validation Loss: 1.3189, Validation Accuracy: 0.5790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2605: 100%|████████████████████████| 1939/1939 [00:13<00:00, 144.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/30, Train Loss: 1.2913, Train Accuracy: 0.6085, Validation Loss: 1.3175, Validation Accuracy: 0.5787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2570: 100%|████████████████████████| 1939/1939 [00:13<00:00, 144.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/30, Train Loss: 1.2905, Train Accuracy: 0.6094, Validation Loss: 1.3149, Validation Accuracy: 0.5835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4017: 100%|████████████████████████| 1939/1939 [00:13<00:00, 140.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/30, Train Loss: 1.2878, Train Accuracy: 0.6125, Validation Loss: 1.3152, Validation Accuracy: 0.5825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4122: 100%|████████████████████████| 1939/1939 [00:13<00:00, 141.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/30, Train Loss: 1.2862, Train Accuracy: 0.6147, Validation Loss: 1.3135, Validation Accuracy: 0.5866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2432: 100%|████████████████████████| 1939/1939 [00:13<00:00, 144.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/30, Train Loss: 1.2840, Train Accuracy: 0.6172, Validation Loss: 1.3147, Validation Accuracy: 0.5826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2249: 100%|████████████████████████| 1939/1939 [00:13<00:00, 142.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/30, Train Loss: 1.2822, Train Accuracy: 0.6185, Validation Loss: 1.3187, Validation Accuracy: 0.5799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.7223: 100%|████████████████████████| 1939/1939 [00:13<00:00, 141.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/30, Train Loss: 1.2807, Train Accuracy: 0.6211, Validation Loss: 1.3124, Validation Accuracy: 0.5868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4073: 100%|████████████████████████| 1939/1939 [00:13<00:00, 140.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/30, Train Loss: 1.2799, Train Accuracy: 0.6216, Validation Loss: 1.3110, Validation Accuracy: 0.5880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2324: 100%|████████████████████████| 1939/1939 [00:13<00:00, 143.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/30, Train Loss: 1.2782, Train Accuracy: 0.6231, Validation Loss: 1.3094, Validation Accuracy: 0.5908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.5506: 100%|████████████████████████| 1939/1939 [00:13<00:00, 143.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/30, Train Loss: 1.2771, Train Accuracy: 0.6243, Validation Loss: 1.3111, Validation Accuracy: 0.5898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.5307: 100%|████████████████████████| 1939/1939 [00:13<00:00, 143.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/30, Train Loss: 1.2756, Train Accuracy: 0.6256, Validation Loss: 1.3115, Validation Accuracy: 0.5873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2383: 100%|████████████████████████| 1939/1939 [00:13<00:00, 141.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/30, Train Loss: 1.2752, Train Accuracy: 0.6258, Validation Loss: 1.3084, Validation Accuracy: 0.5916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 0.9155: 100%|████████████████████████| 1939/1939 [00:13<00:00, 141.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/30, Train Loss: 1.2727, Train Accuracy: 0.6288, Validation Loss: 1.3101, Validation Accuracy: 0.5887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4044: 100%|████████████████████████| 1939/1939 [00:13<00:00, 140.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/30, Train Loss: 1.2729, Train Accuracy: 0.6286, Validation Loss: 1.3064, Validation Accuracy: 0.5919\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2961: 100%|████████████████████████| 1939/1939 [00:13<00:00, 143.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/30, Train Loss: 1.2708, Train Accuracy: 0.6311, Validation Loss: 1.3081, Validation Accuracy: 0.5927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2534: 100%|████████████████████████| 1939/1939 [00:13<00:00, 139.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/30, Train Loss: 1.2702, Train Accuracy: 0.6312, Validation Loss: 1.3068, Validation Accuracy: 0.5921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.5258: 100%|████████████████████████| 1939/1939 [00:13<00:00, 140.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/30, Train Loss: 1.2701, Train Accuracy: 0.6319, Validation Loss: 1.3093, Validation Accuracy: 0.5889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.6488: 100%|████████████████████████| 1939/1939 [00:13<00:00, 139.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/30, Train Loss: 1.2685, Train Accuracy: 0.6334, Validation Loss: 1.3080, Validation Accuracy: 0.5917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.4044: 100%|████████████████████████| 1939/1939 [00:13<00:00, 141.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/30, Train Loss: 1.2675, Train Accuracy: 0.6341, Validation Loss: 1.3101, Validation Accuracy: 0.5893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.0939: 100%|████████████████████████| 1939/1939 [00:13<00:00, 141.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/30, Train Loss: 1.2676, Train Accuracy: 0.6335, Validation Loss: 1.3111, Validation Accuracy: 0.5890\n"
          ]
        }
      ],
      "source": [
        "input_size, hidden_sizes, output_size = len(train_vectors[0]), [150 , 50], 5\n",
        "nn_trainer = NeuralNetworkTrainer(input_size, hidden_sizes, output_size,num_epochs=30)\n",
        "nn_trainer.train_nn(train_vectors, y_train,val_vectors, y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOG5siI0duw-",
        "outputId": "667f8f67-4909-4cff-d9e9-4ccc6b95173d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       705\n",
            "           1       0.45      0.43      0.44      2782\n",
            "           2       0.68      0.80      0.73      7817\n",
            "           3       0.45      0.49      0.47      3262\n",
            "           4       0.00      0.00      0.00       939\n",
            "\n",
            "    accuracy                           0.58     15505\n",
            "   macro avg       0.31      0.34      0.33     15505\n",
            "weighted avg       0.52      0.58      0.55     15505\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "preds_ft = nn_trainer.predict(test_vectors)\n",
        "print(classification_report(y_test, preds_ft))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6juAprsPu_cZ"
      },
      "source": [
        "## Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBeYsXLktMJ0",
        "outputId": "696b116d-a775-4867-c6f3-3e79250857ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.read_csv('pr-test-data.csv')\n",
        "testt_phrases = test_df['Phrase'].apply(lambda x: preprocess_text(x))\n",
        "testt_phrases = testt_phrases.apply(lambda x: ' '.join(x))\n",
        "sentiment_nb = nn_trainer.predict(ftv.vectorize(testt_phrases))\n",
        "test_df['Sentiment'] = sentiment_nb\n",
        "\n",
        "ttttt = test_df.drop(['SentenceId','Phrase','PhraseId'],axis=1)\n",
        "ttttt['ID'] = test_df.index + 1\n",
        "ttttt.to_csv('nn_cv.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
